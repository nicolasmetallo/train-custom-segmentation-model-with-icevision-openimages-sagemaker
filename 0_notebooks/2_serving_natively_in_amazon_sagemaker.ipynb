{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train custom segmentation model with `IceVision`, `OpenImages`, and `SageMaker`\n",
    "## Serving PyTorch Models In Production Natively With Amazon Sagemaker\n",
    "\n",
    "Sources:\n",
    "- https://torchserve-on-aws.workshop.aws/en/100-introduction.html\n",
    "- https://github.com/aws-samples/amazon-sagemaker-endpoint-deployment-of-fastai-model-with-torchserve"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Your Hosting Environment\n",
    "The focus of this lab is around model serving. In that vain, we have taken care of of the data preparation and model training. \n",
    "This lab exercise is using a [HuggingFace Transformer](https://huggingface.co/transformers/) which provides us with a general-purpose architecture for Natural Language Understanding (NLU). Specifically, we are presenting you with a [RoBERTa base](https://huggingface.co/roberta-base) transformer that was fined tuned to perform sentiment analysis. The pre-trained checkpoint loads the additional head layers and will output ``positive``, ``neutral``, and ``negative`` sentiment or text. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import datetime\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor, json_serializer, json_deserializer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "client = boto3.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "name = 'maskrcnn-background-remover'\n",
    "\n",
    "training_jobs = client.list_training_jobs(\n",
    "    NameContains='mask-rcnn',\n",
    "    StatusEquals='Failed',\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending',\n",
    ")\n",
    "\n",
    "training_job_name = training_jobs['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "model_artifact = f's3://{bucket}/{training_job_name}/output/model.tar.gz'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Your Endpoint\n",
    "We will now create and deploy our model. To begin, we need to construct a new PyTorchModel object which points to the pre-trained model artifacts from the above step and also points to the inference code that we wish to use. We will then call the deploy method to launch the deployment container on our TorchServe powered Amazon SageMaker endpoint."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "class ImageSegmenter(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super().__init__(endpoint_name, sagemaker_session=sagemaker_session, \n",
    "                         serializer=json_serializer, deserializer=json_deserializer)\n",
    "\n",
    "# Create SageMaker model and deploy an endpoint\n",
    "sm_pytorch_compiled_model = PyTorchModel(\n",
    "    model_data=model_artifact,\n",
    "    name=name_from_base(f'{name}-torchserve'),\n",
    "    role=role,\n",
    "    entry_point='torchserve-predictor.py',\n",
    "    source_dir='../2_deployment_code/serving_natively_with_amazon_sagemaker',\n",
    "    framework_version='1.7.1',\n",
    "    py_version='py36',\n",
    "    predictor_cls=ImageSegmenter,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "source": [
    "# It will take around 7 minutes for your TorchServe powered endpoint to spin up on Amazon SageMaker \n",
    "endpoint_name = name_from_base(f'{name}-model')\n",
    "instance_type='ml.m5.xlarge'\n",
    "# instance_type='local'\n",
    "\n",
    "predictor = sm_pytorch_compiled_model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Attaching to hdh0k57bfc-algo-1-rjgi1\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (1.19.1)\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:52,844 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Torchserve version: 0.3.0\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Current directory: /\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Number of CPUs: 4\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Max heap size: 1908 M\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Log dir: /logs\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Netty threads: 0\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Netty client threads: 0\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Default workers per model: 4\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Enable metrics API: true\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:52,874 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,348 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag badf3c87f2984f0ba9449b99b218fd89\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,359 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,380 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,529 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,531 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]55\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,531 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,533 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,539 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,540 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,543 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]53\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,544 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,544 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,544 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,548 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,549 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]52\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,550 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,553 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,552 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,599 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,600 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,606 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,606 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,606 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,607 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,608 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,608 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]54\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,610 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,611 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,611 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:55,614 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m Model server started.\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,124 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:04ce76da4041,timestamp:1617734936\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,125 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:0.6897392272949219|#Level:Host|#hostname:04ce76da4041,timestamp:1617734936\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,126 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:102.44019317626953|#Level:Host|#hostname:04ce76da4041,timestamp:1617734936\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,126 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:99.3|#Level:Host|#hostname:04ce76da4041,timestamp:1617734936\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,127 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5927.46484375|#Level:Host|#hostname:04ce76da4041,timestamp:1617734936\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,136 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:1424.00390625|#Level:Host|#hostname:04ce76da4041,timestamp:1617734936\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,137 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:22.3|#Level:Host|#hostname:04ce76da4041,timestamp:1617734936\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,772 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Device on initialisation is: cpu\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,773 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Device on initialisation is: cpu\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,774 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Device on initialisation is: cpu\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:56,791 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Device on initialisation is: cpu\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,721 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,721 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,727 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,727 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,735 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,736 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,803 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,803 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,830 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,831 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,838 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,916 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,930 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   2%|▏         | 2.19M/97.8M [00:00<00:04, 21.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,932 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   2%|▏         | 2.00M/97.8M [00:00<00:04, 20.2MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:57,938 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   3%|▎         | 2.62M/97.8M [00:00<00:03, 27.0MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,020 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   3%|▎         | 2.89M/97.8M [00:00<00:03, 26.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,038 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   6%|▌         | 5.87M/97.8M [00:00<00:03, 31.0MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,042 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   6%|▌         | 5.48M/97.8M [00:00<00:03, 29.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,059 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   6%|▌         | 5.56M/97.8M [00:00<00:03, 29.2MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,148 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   9%|▉         | 9.03M/97.8M [00:00<00:02, 31.5MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,192 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  10%|▉         | 9.45M/97.8M [00:00<00:02, 32.8MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,256 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   9%|▊         | 8.35M/97.8M [00:00<00:03, 26.7MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,269 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   9%|▉         | 9.20M/97.8M [00:00<00:01, 47.8MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,284 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  12%|█▏        | 12.0M/97.8M [00:00<00:02, 30.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,293 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  13%|█▎        | 12.6M/97.8M [00:00<00:03, 27.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,360 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  11%|█         | 10.9M/97.8M [00:00<00:04, 19.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,399 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  15%|█▌        | 15.0M/97.8M [00:00<00:03, 27.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,452 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  16%|█▌        | 15.4M/97.8M [00:00<00:03, 27.8MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,460 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  13%|█▎        | 13.0M/97.8M [00:00<00:04, 20.0MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,487 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  14%|█▍        | 13.8M/97.8M [00:00<00:03, 29.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,569 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  18%|█▊        | 17.6M/97.8M [00:00<00:03, 26.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,576 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  19%|█▊        | 18.1M/97.8M [00:00<00:03, 23.8MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,589 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  18%|█▊        | 17.2M/97.8M [00:00<00:03, 23.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,603 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  17%|█▋        | 16.3M/97.8M [00:00<00:03, 24.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,670 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██        | 20.2M/97.8M [00:00<00:03, 21.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,677 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██        | 20.5M/97.8M [00:00<00:03, 22.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,691 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██        | 20.3M/97.8M [00:00<00:03, 25.2MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,786 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  24%|██▍       | 23.3M/97.8M [00:00<00:03, 24.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,810 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  24%|██▎       | 23.2M/97.8M [00:00<00:02, 26.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,850 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  23%|██▎       | 22.5M/97.8M [00:00<00:03, 22.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,851 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  19%|█▉        | 18.7M/97.8M [00:00<00:03, 21.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,886 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  26%|██▋       | 25.7M/97.8M [00:01<00:03, 24.0MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,914 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  27%|██▋       | 26.0M/97.8M [00:01<00:02, 25.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,952 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  21%|██▏       | 21.0M/97.8M [00:01<00:05, 15.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:58,954 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  25%|██▌       | 24.7M/97.8M [00:01<00:04, 18.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,026 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  31%|███       | 30.0M/97.8M [00:01<00:02, 29.7MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,057 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  27%|██▋       | 26.6M/97.8M [00:01<00:03, 18.8MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,061 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  24%|██▍       | 23.3M/97.8M [00:01<00:04, 17.7MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,075 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  31%|███▏      | 30.7M/97.8M [00:01<00:02, 31.7MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,128 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  34%|███▎      | 32.9M/97.8M [00:01<00:02, 26.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,159 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  31%|███▏      | 30.8M/97.8M [00:01<00:02, 25.0MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,161 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  27%|██▋       | 26.6M/97.8M [00:01<00:03, 21.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,180 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  35%|███▍      | 34.0M/97.8M [00:01<00:02, 27.8MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,231 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  36%|███▋      | 35.6M/97.8M [00:01<00:02, 27.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,261 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  33%|███▎      | 32.3M/97.8M [00:01<00:02, 30.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,272 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  38%|███▊      | 37.1M/97.8M [00:01<00:01, 35.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,286 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  38%|███▊      | 36.9M/97.8M [00:01<00:02, 28.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,332 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  41%|████      | 40.1M/97.8M [00:01<00:01, 32.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,414 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  43%|████▎     | 41.7M/97.8M [00:01<00:01, 33.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,451 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  44%|████▍     | 43.5M/97.8M [00:01<00:01, 33.2MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,464 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  40%|███▉      | 39.1M/97.8M [00:01<00:01, 41.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,514 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  46%|████▌     | 45.0M/97.8M [00:01<00:01, 31.5MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,554 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  48%|████▊     | 46.7M/97.8M [00:01<00:01, 31.7MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,589 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  42%|████▏     | 40.7M/97.8M [00:01<00:01, 35.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,617 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  50%|████▉     | 48.7M/97.8M [00:01<00:01, 33.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,631 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  44%|████▍     | 43.5M/97.8M [00:01<00:01, 33.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,654 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  52%|█████▏    | 50.4M/97.8M [00:01<00:01, 33.3MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,733 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  48%|████▊     | 47.2M/97.8M [00:01<00:01, 29.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,739 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  45%|████▌     | 44.3M/97.8M [00:01<00:02, 22.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,765 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  55%|█████▌    | 53.8M/97.8M [00:01<00:01, 34.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,778 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  53%|█████▎    | 52.3M/97.8M [00:01<00:01, 34.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,835 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  52%|█████▏    | 50.5M/97.8M [00:01<00:01, 30.8MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,841 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  48%|████▊     | 47.1M/97.8M [00:02<00:02, 21.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,914 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  58%|█████▊    | 57.1M/97.8M [00:02<00:01, 33.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,966 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  58%|█████▊    | 56.2M/97.8M [00:02<00:01, 37.7MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:48:59,971 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  57%|█████▋    | 55.6M/97.8M [00:01<00:01, 29.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,030 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  58%|█████▊    | 56.5M/97.8M [00:02<00:01, 37.4MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,063 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  62%|██████▏   | 60.3M/97.8M [00:02<00:01, 29.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,088 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  60%|█████▉    | 58.6M/97.8M [00:02<00:01, 24.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,110 [WARN ] W-9000-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  62%|██████▏   | 60.5M/97.8M [00:02<00:01, 36.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,149 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  63%|██████▎   | 61.1M/97.8M [00:02<00:01, 33.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,188 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  62%|██████▏   | 61.1M/97.8M [00:02<00:01, 23.7MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,205 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  65%|██████▍   | 63.2M/97.8M [00:02<00:01, 26.1MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,298 [WARN ] W-9001-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  67%|██████▋   | 65.1M/97.8M [00:02<00:01, 33.5MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,303 [WARN ] W-9003-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  67%|██████▋   | 65.3M/97.8M [00:02<00:01, 28.6MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,305 [WARN ] W-9002-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -  67%|██████▋   | 65.8M/97.8M [00:02<00:01, 23.9MB/s]\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,332 [INFO ] pool-1-thread-5 ACCESS_LOG - /172.18.0.1:49852 \"GET /ping HTTP/1.1\" 200 12\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:00,333 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:04ce76da4041,timestamp:null\n",
      "!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "import requests\n",
    "import io\n",
    "import time\n",
    "import base64\n",
    "\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "def get_image_bytes(url_or_path:str):\n",
    "    try:\n",
    "        response = requests.get(url_or_path)\n",
    "        data = urlopen(url_or_path)\n",
    "    except Exception:\n",
    "        data = open(url_or_path,'rb')\n",
    "    return data.read()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "path_to_image = 'https://df2sm3urulav.cloudfront.net/tenants/ca/uploads/images/0-4999/1601/5d82a21c1abf4.jpg'\n",
    "# path_to_image = 'test_images/FindID_161098.jpg'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "payload = get_image_bytes(path_to_image)\n",
    "inference_response = predictor.predict(data=base64.b64encode(payload).decode('utf-8'), \n",
    "                                       initial_args = {\"ContentType\": \"application/json\"})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The json_serializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:57,636 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:57,636 [INFO ] W-9003-model_1 ACCESS_LOG - /172.18.0.1:49878 \"POST /invocations HTTP/1.1\" 500 2\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:57,636 [INFO ] W-9003-model_1 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:04ce76da4041,timestamp:null\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:57,637 [INFO ] W-9003-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.45|#ModelName:model,Level:Model|#hostname:04ce76da4041,requestID:4865609d-9e4c-4e28-b7fc-f4af234a93fc,timestamp:1617734997\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:57,637 [INFO ] W-9003-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:04ce76da4041,timestamp:null\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:49:57,637 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:04ce76da4041,timestamp:null\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-16fbc5cadc69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m inference_response = predictor.predict(data=base64.b64encode(payload).decode('utf-8'), \n\u001b[0;32m----> 3\u001b[0;31m                                        initial_args = {\"ContentType\": \"application/json\"})\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mresponse_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ContentType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"application/octet-stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     def _create_request_args(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/deprecations.py\u001b[0m in \u001b[0;36mdeprecate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mrenamed_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/deserializers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(self, stream, content_type)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "with open('test_images/FindID_161098.jpg', \"rb\") as image_file:\n",
    "    img_data = base64.b64encode(image_file.read())\n",
    "    data = {\"img_id\": 1}\n",
    "    data[\"img_data\"] = img_data.decode('utf-8')\n",
    "    body=json.dumps(data).encode('utf-8')\n",
    "    \n",
    "response = client.invoke_endpoint(EndpointName='maskrcnn-background-remover-model-2021-04-06-18-48-26-833',\n",
    "                                  ContentType=\"application/json\",\n",
    "                                  Accept=\"application/json\",\n",
    "                                  Body=body)\n",
    "body=response['Body'].read()\n",
    "msg=body.decode('utf-8')\n",
    "data=json.loads(msg)\n",
    "assert data is not None"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint maskrcnn-background-remover-model-2021-04-06-18-48-26-833 of account 849118573017 not found.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-7fb5b06abde6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                   \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                   \u001b[0mAccept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                   Body=body)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint maskrcnn-background-remover-model-2021-04-06-18-48-26-833 of account 849118573017 not found."
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:52:56,047 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:04ce76da4041,timestamp:1617735176\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:52:56,047 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:0.5941810607910156|#Level:Host|#hostname:04ce76da4041,timestamp:1617735176\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:52:56,047 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:102.53575134277344|#Level:Host|#hostname:04ce76da4041,timestamp:1617735176\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:52:56,047 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:99.4|#Level:Host|#hostname:04ce76da4041,timestamp:1617735176\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:52:56,048 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4580.80859375|#Level:Host|#hostname:04ce76da4041,timestamp:1617735176\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:52:56,048 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:2770.25|#Level:Host|#hostname:04ce76da4041,timestamp:1617735176\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:52:56,048 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:39.9|#Level:Host|#hostname:04ce76da4041,timestamp:1617735176\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:53:56,046 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:04ce76da4041,timestamp:1617735236\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:53:56,046 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:0.5941810607910156|#Level:Host|#hostname:04ce76da4041,timestamp:1617735236\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:53:56,047 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:102.53575134277344|#Level:Host|#hostname:04ce76da4041,timestamp:1617735236\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:53:56,047 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:99.4|#Level:Host|#hostname:04ce76da4041,timestamp:1617735236\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:53:56,047 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:4580.03515625|#Level:Host|#hostname:04ce76da4041,timestamp:1617735236\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:53:56,047 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:2771.0234375|#Level:Host|#hostname:04ce76da4041,timestamp:1617735236\n",
      "\u001b[36mhdh0k57bfc-algo-1-rjgi1 |\u001b[0m 2021-04-06 18:53:56,047 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:39.9|#Level:Host|#hostname:04ce76da4041,timestamp:1617735236\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Cleanup: Delete Endpoint, Endpoint Configuration, and Model\n",
    "In order to ensure that we are no longer being billed for the endpoint or it's associated resrouces that we have spun up, we use the below steps to tear the environment down. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "predictor.delete_model()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Congratulations!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}